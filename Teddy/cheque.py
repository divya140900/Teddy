# -*- coding: utf-8 -*-
"""Cheque.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JW-Cp3kIFn5IxTC6eSelzQpfwpIRA5cr
"""

import cv2
import shutil
import imutils
from imutils import contours
from PIL import Image
import numpy as np
import os
import pytesseract as tr 
import re 
import os 
import io 
from matplotlib import pyplot as plt 
import pandas as pd 
from skimage.segmentation import clear_border
from imutils import contours
from keras.models import load_model
# from tensorflow.keras.models import load_model
from imutils.contours import sort_contours
import imutils
from skimage.segmentation import clear_border
from imutils import contours
import numpy as np
import argparse
import imutils
import cv2
from PIL import Image
from numpy import asarray
import csv
import fitz # PyMuPDF
import io
from PIL import Image 
import PIL 

rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (17, 7))

def start_pos(image, restart):
    """Function to find start point of the Non - White pixel"""
    for i in range(restart, image.shape[0]):
        if 0 in image[i]:
            return i


def stop_pos(image, start):
    """Function to find Stop point of the Non - White row"""
    stop = None
    for i in range(start, image.shape[0]):
        if 0 not in image[i]:
            stop = i
            break
    if stop is None:
        stop = image.shape[0]
        return stop
    else:
        return stop

		
def strt_stp_pos_image(bw_image):
    """this function is used for padding for tracking start and stop points"""
#     bw_image = np.array(bw_image)
    restart = 0
    start_pos_arr = []
    stop_pos_arr = []
    image_last_row = 0

    for i in range(bw_image.shape[0] - 1, 0, -1):
        if 0 in bw_image[i]:
            image_last_row = i
            break

    while restart < image_last_row + 1:
        start_position = start_pos(bw_image, restart)  
        stop_position = stop_pos(bw_image, start_position)  
        if abs(start_position - stop_position) > 10:
            start_pos_arr.append(start_position)
            stop_pos_arr.append(stop_position)
        # Added below if condition to handle image which have signature till last row
        if stop_position is None:
            stop_position = image_last_row
        restart = stop_position
        if restart == image_last_row:
            break
    return start_pos_arr, stop_pos_arr

	
def start(image):
	"""Function to find start point of the Non - White pixel"""
	for i in range(image.shape[0]):
		if 0 in image[i]:
			return i


def stop(image):
	"""Function to find Stop point of the Non - White row"""
	for i in range(image.shape[0]-1, 0, -1):
		if 0 in image[i]:
			return i
			
def detect_horizontal_line(image):
    edges = cv2.Canny(image,50,150,apertureSize = 3) 
    lines = cv2.HoughLines(edges,1,np.pi/2, int(image.shape[1] * 0.5))
    if lines is not None:
        print("Horizontal line detected")
        horizontal_line_indicator = True
        for i in  range(0,lines.shape[0]):
            r,theta = lines[i,0]
            a = np.cos(theta) 
            b = np.sin(theta) 
            x0 = a*r 
            y0 = b*r 
            x1 = int(x0 + 1000*(-b)) 
            y1 = int(y0 + 1000*(a)) 
            x2 = image.shape[1]
            y2 = y1
    # --Draws a complete horizontal line ------ #
            cv2.line(image,(x1,y1), (x2,y2), (0,0,255),3)
    return image

def correct_line(image):
	"""this function preprocesses the form, removes all the lines across the form"""	
	binary_img = cv2.bitwise_not(image)
	binary_image = np.copy(binary_img)

	col = binary_image.shape[1]

	kernel_size = int(col / 80)
#	  v_kernel_size = int(col / 40)
	v_kernel_size = 20
#	  kernel_size = 20
#	  kernel_size = 10
#	  print('kernel size->', kernel_size)
	# ----Create horizontal and vertical line mask -- #

	horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_size, 1))
	vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, v_kernel_size))
	horizontal_temp = cv2.erode(binary_image, horizontal_kernel, iterations = 3)
	horizontal_mask = cv2.dilate(horizontal_temp, horizontal_kernel, iterations = 3)
	vertical_temp = cv2.erode(binary_image, vertical_kernel, iterations =3)
	vertical_mask = cv2.dilate(vertical_temp, vertical_kernel, iterations =3)
#	  cv2.imwrite('init_hor_mask.jpg', horizontal_mask)
#	  cv2.imwrite('init_ver_mask.jpg', vertical_mask)
	horizontal_line = np.copy(horizontal_mask)
	vertical_line = np.copy(vertical_mask)

	horizontal_line = cv2.threshold(horizontal_line, 0, 255, cv2.THRESH_OTSU)[1]
	vertical_line = cv2.threshold(vertical_line, 0, 255, cv2.THRESH_OTSU)[1]

	# -- Store final horizontal & vertical mask -- #
	# -- in the below numpy arrays				-- #

	horizontal_line_cpy = np.zeros_like(horizontal_line)
	vertical_line_cpy = np.zeros_like(vertical_line)

	horizontal_contours = cv2.findContours(horizontal_line, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[1]
	vertical_contours = cv2.findContours(vertical_line, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[1]

	try:
		cv2.drawContours(horizontal_line_cpy, horizontal_contours, -1, (255, 255, 255), 2)
		cv2.fillPoly(horizontal_line_cpy, pts = horizontal_contours, color = (255, 255, 255))
	except Exception:
		print("No horizontal contours detected ")
	
	try:
		cv2.drawContours(vertical_line_cpy, vertical_contours, -1, (255, 255, 255), 2)
		cv2.fillPoly(vertical_line_cpy, pts = vertical_contours, color = (255, 255, 255))
	except Exception:
		print("No verticals contours detected ")
		
	horizontal_line = np.copy(horizontal_line_cpy)
	vertical_line = np.copy(vertical_line_cpy)
	horizontal_line_mask = np.copy(horizontal_line_cpy)
	vertical_line_mask = np.copy(vertical_line_cpy)
	horizontal_line = cv2.bitwise_not(horizontal_line)
	vertical_line = cv2.bitwise_not(vertical_line)
	
	final_mask = cv2.bitwise_or(vertical_line, vertical_line, mask = horizontal_line)

	cv2.imwrite('./../final_mask.jpg', final_mask)
	final_mask = cv2.threshold(final_mask, 0, 255, cv2.THRESH_OTSU)[1]

	res = cv2.bitwise_and(binary_image, binary_image, mask = final_mask)
	res = cv2.bitwise_not(cv2.threshold(res, 0, 255, cv2.THRESH_OTSU)[1])

	# -- For filling lost features -- #
	temp = cv2.inpaint(res, horizontal_line_mask, 3, cv2.INPAINT_TELEA)
	line_corrected_img = cv2.inpaint(temp, vertical_line_mask, 3, cv2.INPAINT_TELEA)
	line_corrected_img = cv2.threshold(line_corrected_img, 0, 255, cv2.THRESH_OTSU)[1]

	return line_corrected_img, final_mask
	
	
def pad_img(img, img_bkp):
	start_pos = start(img)
	stop_pos = stop(img)
	if (start_pos - 10) >= 0:
		start_pos = start_pos - 10
	else:
		start_pos = start_pos
	if (stop_pos + 10) <= img.shape[0]:
		stop_pos = stop_pos + 10
	else:
		stop_pos = stop_pos
	
	pad_img = img[start_pos:stop_pos]
	pad_img_bkp = img_bkp[start_pos:stop_pos]
	
	transpose = cv2.transpose(pad_img)
	transpose_bkp = cv2.transpose(pad_img_bkp)
	start_pos = start(transpose)
	stop_pos = stop(transpose)
	if (start_pos - 10) >= 0:
		start_pos = start_pos - 10
	else:
		start_pos = start_pos
	if (stop_pos + 10) <= img.shape[0]:
		stop_pos = stop_pos + 10
	else:
		stop_pos = stop_pos
	pad_img = transpose[start_pos:stop_pos]
	pad_img_bkp = transpose_bkp[start_pos:stop_pos]
	padded_img = cv2.transpose(pad_img)
	padded_img_bkp = cv2.transpose(pad_img_bkp)
	return padded_img, padded_img_bkp

def basic_ocr_to_text(image):
    image = cv2.imread(image)
    if image is None:
        print("Could not open or find the image: ")
        exit(0)

    alpha = 1.4  # contrast control
    beta = 0  # brightness control

    print("OUTPUT BEGINS")
    print("-------------------------")

    # grayscaling contrasted image
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    # thresholding image
    ret, thresh1 = cv2.threshold(gray_image, 127, 255, cv2.THRESH_BINARY)
    cv2.waitKey(0)
    # saving image
    cv2.imwrite("C:/Users/Divya/OneDrive/Desktop/teddy/static/check_images/image2_2.png", thresh1)
    cv2.waitKey(0)

    # tesseract configuration
    config = "-l eng --oem 1 --psm 3"

    # Read image from disk
    im = cv2.imread(
        "C:/Users/Divya/OneDrive/Desktop/teddy/static/check_images/image2_2.png", cv2.IMREAD_COLOR
    )

    # tesseract OCR on image
    text = tr.image_to_string(im, config=config)
    # print(text)

    # Create a black image
    img = np.zeros((1924, 1024, 3), np.uint8)

    text3 = repr(text)
    font = cv2.FONT_HERSHEY_COMPLEX
    bottomLeftCornerOfText = (10, 50)
    fontScale = 1
    fontColor = (255, 255, 255)
    lineType = 2

    return text3



def ext_ocr_details(img):	
	# ------ connected components ---- ###
			text = basic_ocr_to_text('C:/Users/Divya/OneDrive/Desktop/teddy/static/check_images/copped_image_for_ifsc.jpg')
			text = text.lower()
			sub = "ifs"
			ifsc = ''
			idx1 = text.index(sub)
			for idx in range(idx1 + len(sub) + 1, idx1 + len(sub) + 1 + 15):
						ifsc = ifsc + text[idx]
			return ifsc

def ext_amount(image, template):

	amount_path = 'C:/Users/Divya/OneDrive/Desktop/teddy/static/check_images'
	if os.path.exists(amount_path) and os.path.isdir(amount_path):
		shutil.rmtree(amount_path)
	img_bkp = np.copy(image)
	if img_bkp.ndim == 2:
		img_bkp = cv2.cvtColor(img_bkp, cv2.COLOR_GRAY2RGB)
	if not os.path.exists(amount_path):
		os.mkdir(amount_path)
	#image = cv2.imread('./preprocessed_img.jpg', 0)
	#template = cv2.imread('rupee_template.jpg', 0)
	#image = cv2.threshold(image, 0, 255, cv2.THRESH_OTSU)[1]
	template = cv2.threshold(template, 0, 255, cv2.THRESH_OTSU)[1]
	
	template = cv2.Canny(template, 50, 200)
	(tH, tW) = template.shape[:2]
	
	# loop over the scales of the image
	found = None
	count = 0
	for scale in np.linspace(0.2, 1.0, 20)[::-1]:
		resized = imutils.resize(image, width = int(image.shape[1] * scale))
		r = image.shape[1] / float(resized.shape[1])
 
		# if the resized image is smaller than the template, then break
		# from the loop
		if resized.shape[0] < tH or resized.shape[1] < tW:
			break
			
		# detect edges in the resized, grayscale image and apply template
		# matching to find the template in the image
		edged = cv2.Canny(resized, 50, 200)
		result = cv2.matchTemplate(edged, template, cv2.TM_CCOEFF)
		(_, maxVal, _, maxLoc) = cv2.minMaxLoc(result)
 
		# check to see if the iteration should be visualized
		# draw a bounding box around the detected region
		clone = np.dstack([edged, edged, edged])
		cv2.rectangle(clone, (maxLoc[0], maxLoc[1]),
			(maxLoc[0] + tW, maxLoc[1] + tH), (0, 0, 255), 2)
		#cv2.imwrite('template_'+str(count)+'.jpg', clone)
		count += 1
		# if we have found a new maximum correlation value, then update
		# the bookkeeping variable
		if found is None or maxVal > found[0]:
			found = (maxVal, maxLoc, r)
 
	# unpack the bookkeeping variable and compute the (x, y) coordinates
	# of the bounding box based on the resized ratio
	(_, maxLoc, r) = found
	(startX, startY) = (int(maxLoc[0] * r), int(maxLoc[1] * r))
	(endX, endY) = (int((maxLoc[0] + tW) * r), int((maxLoc[1] + tH) * r))
 
	# draw a bounding box around the detected result and display the image
	cv2.rectangle(img_bkp, (startX, startY), (endX, endY), (0, 0, 255), 2)
	cv2.imwrite('C:/Users/Divya/OneDrive/Desktop/teddy/static/check_images/final_templ.jpg', img_bkp)
	# print('startX->', startX, 'startY->', startY, 'endX->', endX, 'endY->', endY)
	amt_x1 = endX  - 10 # bottom_right[0] + 10
	amt_y1 = startY - 10 # top_left[1] - 10
	amt_x2 = endX + 30 # bottom_right[0] + 10
	amt_y2 = endY + 10 # bottom_right[1] + 10
	h = abs(amt_y2 - amt_y1)
	y = amt_y1
	x = amt_x2 + 10
	w = abs(x - image.shape[1])
	#print('y->', y, 'y+h->', y+h, 'x->', x, 'x+w->', x+w)
	amount = image[y:y+h, x:x+w-150]
	padded_img, padded_img_bkp = pad_img(amount, amount)
	padded_img = cv2.copyMakeBorder(padded_img, top=5, bottom=5, left=5, right=5, borderType= cv2.BORDER_CONSTANT, value=[255,255,255])
	padded_img_bkp = cv2.copyMakeBorder(padded_img_bkp, top=5, bottom=5, left=5, right=5, borderType= cv2.BORDER_CONSTANT, value=[255,255,255])
	cv2.imwrite('C:/Users/Divya/OneDrive/Desktop/teddy/static/check_images/padded_amount.jpg', padded_img)
	amount = handwriting_recognition('C:/Users/Divya/OneDrive/Desktop/teddy/static/check_images/padded_amount.jpg')
	# print('amount->', "".join(amount))
	amount = "".join(amount)
	return amount

# load the input image from disk, convert it to grayscale, and blur
# it to reduce noise


def handwriting_recognition(image):
    # print("[INFO] loading handwriting OCR model...")
    model = load_model("C:/Users/Divya/OneDrive/Desktop/teddy/models/handwriting_recognition_final_model")
    image = cv2.imread(image)
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    blurred = cv2.GaussianBlur(gray, (5, 5), 0)
    # perform edge detection, find contours in the edge map, and sort the
    # resulting contours from left-to-right
    edged = cv2.Canny(blurred, 30, 150)
    cnts = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts = imutils.grab_contours(cnts)
    cnts = sort_contours(cnts, method="left-to-right")[0]
    # initialize the list of contour bounding boxes and associated
    # characters that we'll be OCR'ing
    chars = []
    # loop over the contours
    prediction = ""
    for c in cnts:
        # compute the bounding box of the contour
        (x, y, w, h) = cv2.boundingRect(c)
        # filter out bounding boxes, ensuring they are neither too small
        # nor too large
        if (w >= 5 and w <= 150) and (h >= 15 and h <= 120):
            # extract the character and threshold it to make the character
            # appear as *white* (foreground) on a *black* background, then
            # grab the width and height of the thresholded image
            roi = gray[y : y + h, x : x + w]
            thresh = cv2.threshold(
                roi, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU
            )[1]
            (tH, tW) = thresh.shape
            # if the width is greater than the height, resize along the
            # width dimension
            if tW > tH:
                thresh = imutils.resize(thresh, width=32)
            # otherwise, resize along the height
            else:
                thresh = imutils.resize(thresh, height=32)
                # re-grab the image dimensions (now that its been resized)
            # and then determine how much we need to pad the width and
            # height such that our image will be 32x32
            (tH, tW) = thresh.shape
            dX = int(max(0, 32 - tW) / 2.0)
            dY = int(max(0, 32 - tH) / 2.0)
            # pad the image and force 32x32 dimensions
            padded = cv2.copyMakeBorder(
                thresh,
                top=dY,
                bottom=dY,
                left=dX,
                right=dX,
                borderType=cv2.BORDER_CONSTANT,
                value=(0, 0, 0),
            )
            padded = cv2.resize(padded, (32, 32))
            # prepare the padded image for classification via our
            # handwriting OCR model
            padded = padded.astype("float32") / 255.0
            padded = np.expand_dims(padded, axis=-1)
            # update our list of characters that will be OCR'd
            chars.append((padded, (x, y, w, h)))
    # extract the bounding box locations and padded characters
    boxes = [b[1] for b in chars]
    chars = np.array([c[0] for c in chars], dtype="float32")
    # OCR the characters using our handwriting recognition model
    preds = model.predict(chars)
    # define the list of label names
    labelNames = "0123456789"
    labelNames += "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
    labelNames = [l for l in labelNames]
    # loop over the predictions and bounding box locations together

    for (pred, (x, y, w, h)) in zip(preds, boxes):
        # find the index of the label with the largest corresponding
        # probability, then extract the probability and label
        i = np.argmax(pred)
        prob = pred[i]
        label = labelNames[i]
        # draw the prediction on the image
        print("[INFO] {} - {:.2f}%".format(label, prob * 100))
        prediction += label
        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)
        cv2.putText(
            image,
            label,
            (x - 10, y - 10),
            cv2.FONT_HERSHEY_SIMPLEX,
            1.2,
            (0, 255, 0),
            2,
        )
        # show the image
        cv2.waitKey(0)
    return prediction

def extract_digits_and_symbols(image, charCnts, minW=5, minH=15):
	# grab the internal Python iterator for the list of character
	# contours, then  initialize the character ROI and location
	# lists, respectively
	charIter = charCnts.__iter__()
	rois = []
	locs = []

	# keep looping over the character contours until we reach the end
	# of the list
	while True:
		try:
			# grab the next character contour from the list, compute
			# its bounding box, and initialize the ROI
			c = next(charIter)
			(cX, cY, cW, cH) = cv2.boundingRect(c)
			roi = None

			# check to see if the width and height are sufficiently
			# large, indicating that we have found a digit
			if cW >= minW and cH >= minH:
				# extract the ROI
				roi = image[cY:cY + cH, cX:cX + cW]
				rois.append(roi)
				locs.append((cX, cY, cX + cW, cY + cH))

			# otherwise, we are examining one of the special symbols
			else:
				# MICR symbols include three separate parts, so we
				# need to grab the next two parts from our iterator,
				# followed by initializing the bounding box
				# coordinates for the symbol
				parts = [c, next(charIter), next(charIter)]
				(sXA, sYA, sXB, sYB) = (np.inf, np.inf, -np.inf,
					-np.inf)

				# loop over the parts
				for p in parts:
					# compute the bounding box for the part, then
					# update our bookkeeping variables
					(pX, pY, pW, pH) = cv2.boundingRect(p)
					sXA = min(sXA, pX)
					sYA = min(sYA, pY)
					sXB = max(sXB, pX + pW)
					sYB = max(sYB, pY + pH)

				# extract the ROI
				roi = image[sYA:sYB, sXA:sXB]
				rois.append(roi)
				locs.append((sXA, sYA, sXB, sYB))

		# we have reached the end of the iterator; gracefully break
		# from the loop
		except StopIteration:
			break

	# return a tuple of the ROIs and locations
	return (rois, locs)

# construct the argument parse and parse the arguments
# ap = argparse.ArgumentParser()
# ap.add_argument("-i", "--image", required=True,
# 	help="path to input image")
# ap.add_argument("-r", "--reference", required=True,
# 	help="path to reference MICR E-13B font")
# args = vars(ap.parse_args())
def extract_micr (image):
	# args = {
	# 				"reference": "C:/Users/Divya/OneDrive/Desktop/teddy/static/Images/micr_e13b_reference (1).png"}

	# initialize the list of reference character names, in the same
	# order as they appear in the reference image where the digits
	# their names and:
	# T = Transit (delimit bank branch routing transit #)
	# U = On-us (delimit customer account number)
	# A = Amount (delimit transaction amount)
	# D = Dash (delimit parts of numbers, such as routing or account)
	charNames = ["1", "2", "3", "4", "5", "6", "7", "8", "9", "0",
		"T", "U", "A", "D"]

	# load the reference MICR image from disk, convert it to grayscale,
	# and threshold it, such that the digits appear as *white* on a
	# *black* background
	ref = cv2.imread('C:/Users/Divya/OneDrive/Desktop/teddy/static/check_images_input/micr_e13b_reference.png')
	ref = cv2.cvtColor(ref, cv2.COLOR_BGR2GRAY)
	ref = imutils.resize(ref, width=400)
	ref = cv2.threshold(ref, 0, 255, cv2.THRESH_BINARY_INV |
		cv2.THRESH_OTSU)[1]

	# find contours in the MICR image (i.e,. the outlines of the
	# characters) and sort them from left to right
	refCnts = cv2.findContours(ref.copy(), cv2.RETR_EXTERNAL,
		cv2.CHAIN_APPROX_SIMPLE)
	refCnts = refCnts[0]
	refCnts = contours.sort_contours(refCnts, method="left-to-right")[0]

	# create a clone of the original image so we can draw on it
	# extract the digits and symbols from the list of contours, then
	# initialize a dictionary to map the character name to the ROI
	refROIs = extract_digits_and_symbols(ref, refCnts,
		minW=10, minH=20)[0]
	chars = {}
	# loop over the reference ROIs
	for (name, roi) in zip(charNames, refROIs):
		# resize the ROI to a fixed size, then update the characters
		# dictionary, mapping the character name to the ROI
		roi = cv2.resize(roi, (36, 36)) 
		chars[name] = roi
	# an empty list to store the output of the check OCR
	rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (17, 7))
	output = []
	# load the input image, grab its dimensions, and apply array slicing
	# to keep only the bottom 20% of the image (that's where the account
	# information is)
	image = cv2.imread(image)
	(h, w,) = image.shape[:2]
	delta = int(h - (h * 0.2))
	bottom = image[delta + 20:h, 0:w]
	# convert the bottom image to grayscale, then apply a blackhat
	# morphological operator to find dark regions against a light
	# background (i.e., the routing and account numbers)
	gray = cv2.cvtColor(bottom, cv2.COLOR_BGR2GRAY)
	blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, rectKernel)
	gradX = cv2.Sobel(blackhat, ddepth=cv2.CV_32F, dx=1, dy=0,
		ksize=-1)
	gradX = np.absolute(gradX)
	(minVal, maxVal) = (np.min(gradX), np.max(gradX))
	gradX = (255 * ((gradX - minVal) / (maxVal - minVal)))
	gradX = gradX.astype("uint8")
	gradX = cv2.morphologyEx(gradX, cv2.MORPH_CLOSE, rectKernel)
	thresh = cv2.threshold(gradX, 0, 255,
		cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]
	thresh = clear_border(thresh)
	# find contours in the thresholded image, then initialize the
	# list of group locations
	groupCnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,
		cv2.CHAIN_APPROX_SIMPLE)
	groupCnts = imutils.grab_contours(groupCnts)
	groupLocs = []
	# loop over the group contours
	for (i, c) in enumerate(groupCnts):
		# compute the bounding box of the contour
		(x, y, w, h) = cv2.boundingRect(c)
		# only accept the contour region as a grouping of characters if
		# the ROI is sufficiently large
		if w > 50 and h > 15:
			groupLocs.append((x, y, w, h))
	# sort the digit locations from left-to-right
	groupLocs = sorted(groupLocs, key=lambda x:x[0])
	for (gX, gY, gW, gH) in groupLocs:
		# initialize the group output of characters
		groupOutput = []
		# extract the group ROI of characters from the grayscale
		# image, then apply thresholding to segment the digits from
		# the background of the credit card
		group = gray[gY - 5:gY + gH + 5, gX - 5:gX + gW + 5]
		group = cv2.threshold(group, 0, 255,
			cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]
		cv2.waitKey(0)
		# find character contours in the group, then sort them from
		# left to right
		charCnts = cv2.findContours(group.copy(), cv2.RETR_EXTERNAL,
			cv2.CHAIN_APPROX_SIMPLE)
		charCnts = imutils.grab_contours(charCnts)
		charCnts = contours.sort_contours(charCnts,
			method="left-to-right")[0]
		# find the characters and symbols in the group
		(rois, locs) = extract_digits_and_symbols(group, charCnts)
		# loop over the ROIs from the group
		for roi in rois:
			# initialize the list of template matching scores and
			# resize the ROI to a fixed size
			scores = []
			roi = cv2.resize(roi, (36, 36))
			# loop over the reference character name and corresponding
			# ROI
			for charName in charNames:
				# apply correlation-based template matching, take the
				# score, and update the scores list
				result = cv2.matchTemplate(roi, chars[charName],
					cv2.TM_CCOEFF)
				(_, score, _, _) = cv2.minMaxLoc(result)
				scores.append(score)
			# the classification for the character ROI will be the
			# reference character name with the *largest* template
			# matching score
			groupOutput.append(charNames[np.argmax(scores)])
		# draw (padded) bounding box surrounding the group along with
		# the OCR output of the group
		cv2.rectangle(image, (gX - 10, gY + delta - 10),
			(gX + gW + 10, gY + gY + delta), (0, 0, 255), 2)
		cv2.putText(image, "".join(groupOutput),
			(gX - 10, gY + delta - 25), cv2.FONT_HERSHEY_SIMPLEX,
			0.95, (0, 0, 255), 3)
		# add the group output to the overall check OCR output
		output.append("".join(groupOutput))
	# display the output check OCR information to the screen
	print("Check OCR: {}".format(" ".join(output)))
	cv2.waitKey(0)
	return output


def handwritten_cheque_data_extraction(input_image):
  im = Image.open(input_image)
  resized_img = im.resize((2365, 1100))
  resized_img.save('C:/Users/Divya/OneDrive/Desktop/teddy/static/check_images/resized_input.jpg')
  img_color = cv2.imread('C:/Users/Divya/OneDrive/Desktop/teddy/static/check_images/resized_input.jpg')
  if img_color.ndim == 3:
      img = cv2.cvtColor(img_color, cv2.COLOR_RGB2GRAY)
  img = cv2.threshold(img, 0, 255, cv2.THRESH_OTSU)[1]
  line_corrected_img, mask = correct_line(img)
  cv2.imwrite('C:/Users/Divya/OneDrive/Desktop/teddy/static/check_images/preprocessed_img.jpg', line_corrected_img)
  numpydata = asarray(line_corrected_img)
  micr_extracted = extract_micr('C:/Users/Divya/OneDrive/Desktop/teddy/static/check_images/resized_input.jpg')
  print('MICR->', micr_extracted)
  template = cv2.imread('C:/Users/Divya/OneDrive/Desktop/teddy/static/check_images_input/rupee_template.jpg', 0)
  amount = ext_amount(line_corrected_img, template)
  acc_no = tr.image_to_string(line_corrected_img[550:630, 350:750])
  cv2.imwrite('C:/Users/Divya/OneDrive/Desktop/teddy/static/check_images/copped_image_for_ifsc.jpg', line_corrected_img[50:300,20:2200])
  ifsc = ext_ocr_details(line_corrected_img[50:300,20:2200])
  cv2.imwrite('C:/Users/Divya/OneDrive/Desktop/teddy/static/check_images/Payee_name.jpg', line_corrected_img[200:300,200:1800])
  payee = handwriting_recognition('C:/Users/Divya/OneDrive/Desktop/teddy/static/check_images/Payee_name.jpg')

  result = [['MICR','Amount','Account_Number','IFSC','Bearer'], [micr_extracted,amount,acc_no,ifsc,payee]]

  csv_file = "C:/Users/Divya/OneDrive/Desktop/teddy/static/check_images/Cheque_result.csv"
  with open(csv_file, 'w') as f:
      # using csv.writer method from CSV package
      write = csv.writer(f)
      write.writerows(result)



def pdf_to_image_for_cheques(file):
# open the file
  pdf_file = fitz.open(file)
  print(file)
  for page_index in range(len(pdf_file)):
      # get the page itself
      page = pdf_file[page_index]
      # get image list
      image_list = page.get_images()
      # printing number of images found in this page
      if image_list:
          print(f"[+] Found a total of {len(image_list)} images in page {page_index}")
      else:
          print("[!] No images found on page", page_index)
      for image_index, img in enumerate(image_list, start=1):
          # get the XREF of the image
          xref = img[0]
          # extract the image bytes
          base_image = pdf_file.extract_image(xref)
          image_bytes = base_image["image"]
          # get the image extension
          image_ext = base_image["ext"]
          # load it to PIL
          image = Image.open(io.BytesIO(image_bytes))
          # save it to local disk
          image.save(open(f"C:/Users/Divya/OneDrive/Desktop/teddy/static/check_images/input_handwritten_cheques.{image_ext}", "wb"))
          handwritten_cheque_data_extraction("C:/Users/Divya/OneDrive/Desktop/teddy/static/check_images/input_handwritten_cheques.jpeg")



